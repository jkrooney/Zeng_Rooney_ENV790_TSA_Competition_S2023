---
title: "TSA Competition"
author: "Angela Zeng & John Rooney"
output:
  html_document:
    df_print: paged
  pdf_document: default
geometry: margin=2.54cm
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,tidy.opts=list(width.cutoff=80), tidy=FALSE) 
```

## Set up
```{r load libraries, message=FALSE}
#Load libraries 
library(readxl)
library(forecast)
library(tseries)
library(smooth)
library(tidyverse)
library(lubridate)
library(Kendall)
library(writexl)
```

##Import and process data 
```{r Data Import and Wrangling}
#import datasets
load_df<- read_excel(path="./Data/load.xlsx",col_names=TRUE)

#Create column with daily load (by averaging hourly load)
load_df$daily_load<- rowMeans(load_df[,3:26])

#Process date column 
load_df$date<- as.Date(load_df$date)
load_df<- load_df %>%
  arrange(date)

#full clean data frame
full_load <- load_df[,c(2,27)]

#create a subset for training purpose (excludes 2010)
train_load<- full_load %>%
  filter(year(date) <= 2009)

#create a subset for testing purpose (only 2010)
test_load<- full_load %>%
  filter(year(date) == 2010)
```

##Create ts object 
```{r Make Training and Test time series}
#full data set
ts_full<- msts(full_load[,2], seasonal.periods =c(7,365.25), start=c(2005,01,01))
#tsclean identifies and replace outliers and missing values using linear interpolation
clean_ts_full <- tsclean(ts_full) 

#excludes 2010, Training set 
ts_train <- msts(train_load[,2], seasonal.periods =c(7,365.25), start=c(2005,01,01))
clean_ts_train <-  tsclean(ts_train)

#only 2010, Test set
ts_test<- msts(test_load[,2], seasonal.periods =c(7,365.25), start=c(2010,01,01))
clean_ts_test <-  tsclean(ts_test)
```

##Plot data 
```{r Initial Plots, ACF and PACF}
#plot the training dataset
ggplot(train_load, aes(x= date, y= daily_load)) +
  geom_line(color= "blue") +
  labs(y='Demand',
       x='Year')+
  scale_x_date(date_breaks = "1 year", date_labels = "%Y")

par(mfrow=c(1,2))
Acf(clean_ts_train,lag.max=40, plot=TRUE)
Pacf(clean_ts_train,lag.max=40, plot=TRUE)
par(mfrow=c(1,1))
```


```{r Decompose, SMK and ADF tests}
#Decompose
decompose_load<- mstl(clean_ts_train)
autoplot(decompose_load)

deseasonal_load <- seasadj(decompose_load) 

#Tests
#if p value > 0.05 then accept null hypothesis, data has a unit root, i.e., stochastic trend
print("Results for ADF test")
print(adf.test(deseasonal_load, alternative = "stationary"))

#if p value < 0.05 then reject null hypothesis, data follow a trend
SMKtest <- SeasonalMannKendall(deseasonal_load)
print("Results for Seasonal Mann Kendall")
print(summary(SMKtest))
```

```{r Model 1 ARIMA}
# Create Model 1 Auto Arima on deseasonalized data, forecast and plot
model1 <- auto.arima(deseasonal_load, max.D=0, max.P=0, max.Q=0)
print(model1)

model1_forecast <- forecast(object = model1, h=424)
plot(model1_forecast)

#AIC of 27337.23
```

```{r Model 2 SARIMA}
#Create Model 2 SARIMA on original data, forecast and plot
model2 <- auto.arima(clean_ts_train)
print(model2)

model2_forecast <- forecast(object = model2, h=424)
plot(model2_forecast)

#AIC of 27951.64
```

```{r Model 3 SES}
#model 3 Simple Exponential Smoothing on original data
model3 <- es(y=clean_ts_train, h=424, holdout=F, silent=F)
print(model3)
checkresiduals(model3)
#AIC of 28079.91

#SHOULD the SES model BE ON DESEAONALIZED DATA INSTEAD?? 
#Model 3.1 SES on deseasonalized data 
model3.1<- es(y=deseasonal_load, h=424, holdout=F, silent=F)
print(model3.1)
checkresiduals(model3.1)

#Model 3.2 Seasonal SES
model3.2 <- es(y=clean_ts_train, model="ZZZ", h= 424, holdout=F, silent=F)
print(model3.2)
checkresiduals(model3.2)

#Model 3 and 3.2 are the same 

```

```{r Accuracy Test Models 1-3}
#test the accuracy of the models 
model1_scores <- accuracy(model1_forecast, clean_ts_test, test=NULL)
model1_scores<- model1_scores[,c("ME","RMSE","MAE","MPE","MAPE")]
model2_scores <- accuracy(model2_forecast, clean_ts_test, test=NULL)
model2_scores<- model2_scores[,c("ME","RMSE","MAE","MPE","MAPE")]
model3_scores <- accuracy(model3$forecast, clean_ts_test, test=NULL)
model3_scores<- model3_scores[,c("ME","RMSE","MAE","MPE","MAPE")]

model3.1_scores <- accuracy(model3.1$forecast, clean_ts_test, test=NULL)
model3.1_scores<- model3.1_scores[,c("ME","RMSE","MAE","MPE","MAPE")]
model3.2_scores <- accuracy(model3.2$forecast, clean_ts_test, test=NULL)
model3.2_scores<- model3.2_scores[,c("ME","RMSE","MAE","MPE","MAPE")]

scores <- as.data.frame(rbind(model1_scores[2,], model2_scores[2,], model3_scores, model3.1_scores,
                              model3.2_scores ))
print(scores)
```

```{r Extract data for first Kaggle submission}
#extract jan and feb from 2011 forecast of model2
answer<- model2_forecast$mean[366:424]
answer<- as.data.frame(answer)
#write_xlsx(answer, "answer1.xlsx")
```

```{r Model 4 SNAIVE}
model4 <- snaive(clean_ts_train, h=424, holdout=FALSE)
checkresiduals(model4)
plot(model4)
```

```{r Model 5 STL + ETS}
#Fit and forecast STL + ETS model to data
model5 <-  stlf(clean_ts_train, h=424)

#Plot foresting results
autoplot(model5)
```

```{r Model 6 ARIMA + FOURIER terms}
#Fit arima model with fourier terms as exogenous regressors
# seasonal = FALSE is the same as P=D=Q=0
# play with K by changing it to K=c(2,2), K=c(2,4), K=c(2,6), etc. The higher the K the longer it will take to converge, because R will try more models.

model6 <- auto.arima(clean_ts_train, seasonal=FALSE, lambda=0, 
                     xreg=fourier(clean_ts_train, K=c(2,12)))

#Forecast with ARIMA fit
#also need to specify h for fourier terms
model6_forecast <- forecast(model6,
                            xreg=fourier(clean_ts_train, K=c(2,12), h=424),
                            h=424) 

#Plot foresting results
autoplot(model6_forecast)
```

```{r Model 6.2 ARIMA + FOURIER terms}
#Fit arima model with fourier terms as exogenous regressors
# seasonal = FALSE is the same as P=D=Q=0
# play with K by changing it to K=c(2,2), K=c(2,4), K=c(2,6), etc. The higher the K the longer it will take to converge, because R will try more models.

model6.2 <- auto.arima(clean_ts_train, seasonal=FALSE, lambda=0, 
                     xreg=fourier(clean_ts_train, K=c(2,6)))

#Forecast with ARIMA fit
#also need to specify h for fourier terms
model6.2_forecast <- forecast(model6.2,
                            xreg=fourier(clean_ts_train, K=c(2,6), h=424),
                            h=424) 

#Plot foresting results
autoplot(model6.2_forecast)+ylim(0,10000) #with ylim, difficult to see the massive confidence interval
```

```{r Model 6.3 ARIMA + FOURIER terms}
#Fit arima model with fourier terms as exogenous regressors
# seasonal = FALSE is the same as P=D=Q=0
# play with K by changing it to K=c(2,2), K=c(2,4), K=c(2,6), etc. The higher the K the longer it will take to converge, because R will try more models.

model6.3 <- auto.arima(clean_ts_train, seasonal=FALSE, lambda=0, 
                     xreg=fourier(clean_ts_train, K=c(2,4)))

#Forecast with ARIMA fit
#also need to specify h for fourier terms
model6.3_forecast <- forecast(model6.3,
                            xreg=fourier(clean_ts_train, K=c(2,4), h=424),
                            h=424) 

#Plot foresting results
autoplot(model6.3_forecast)+ylim(0,10000) #with ylim, difficult to see the massive confidence interval
```

```{r Model 6.4 ARIMA + FOURIER terms}
#Fit arima model with fourier terms as exogenous regressors
# seasonal = FALSE is the same as P=D=Q=0
# play with K by changing it to K=c(2,2), K=c(2,4), K=c(2,6), etc. The higher the K the longer it will take to converge, because R will try more models.

model6.4 <- auto.arima(clean_ts_train, seasonal=FALSE, lambda=0, 
                     xreg=fourier(clean_ts_train, K=c(2,2)))

#Forecast with ARIMA fit
#also need to specify h for fourier terms
model6.4_forecast <- forecast(model6.4,
                            xreg=fourier(clean_ts_train, K=c(2,2), h=424),
                            h=424) 

#Plot foresting results
autoplot(model6.4_forecast) #with ylim, difficult to see the massive confidence interval
```

```{r Scores2}
#Model 4: Seasonal naive 
model4_scores <- accuracy(model4$mean,clean_ts_test)

#Model 5: STL + ETS
model5_scores <- accuracy(model5$mean, clean_ts_test)  

#Model 6: ARIMA + Fourier 
model6_scores <- accuracy(model6_forecast$mean, clean_ts_test)

model6.2_scores <- accuracy(model6.2_forecast$mean, clean_ts_test)
model6.3_scores <- accuracy(model6.3_forecast$mean, clean_ts_test)
model6.4_scores <- accuracy(model6.3_forecast$mean, clean_ts_test)
```

```{r Accuracy Test Models 1-6}
model4_scores<- model4_scores[,c("ME","RMSE","MAE","MPE","MAPE")]
model5_scores<- model5_scores[,c("ME","RMSE","MAE","MPE","MAPE")]
model6_scores<- model6_scores[,c("ME","RMSE","MAE","MPE","MAPE")]
model6.2_scores<- model6.2_scores[,c("ME","RMSE","MAE","MPE","MAPE")]
model6.3_scores<- model6.3_scores[,c("ME","RMSE","MAE","MPE","MAPE")]
model6.4_scores<- model6.4_scores[,c("ME","RMSE","MAE","MPE","MAPE")]

scores2 <- as.data.frame(rbind(model4_scores, model5_scores, model6_scores, model6.2_scores, model6.3_scores, model6.4_scores))

scores<- rbind(scores,scores2)
print(scores)

#looks like models 6.3 and 6.4 are the same and the best models
```

```{r Extract data for second Kaggle submission}
#extract jan and feb from 2011 forecast of model6.3
answer2<- model6.3_forecast$mean[366:424]
answer2<- as.data.frame(answer2)
#write_xlsx(answer2, "answer2.xlsx")
```


```{r Model 7 TBATS}
model7 <- tbats(clean_ts_train)

model7_forecast <- forecast(model7, h=424)

#Plot foresting results
autoplot(model7_forecast) +
  ylab("load") 

#Plot model + observed data
autoplot(clean_ts_test) +
  autolayer(model7_forecast, series="TBATS",PI=FALSE)+
  ylab("load") 

```

```{r Model 8 Neural Network}
#You can play with the different values for p and P, you can also use xreg with Fourier term to model the multiple seasonality

model8 <- nnetar(clean_ts_train,p=1,P=1,)
model8_forecast <- forecast(model8, h=424) 

model8.1 <- nnetar(clean_ts_train,p=1,P=0,xreg=fourier(clean_ts_train, K=c(2,2)))
model8.1_forecast <- forecast(model8.1, h=424,xreg=fourier(clean_ts_train, K=c(2,2),h=424))

model8.2 <- nnetar(clean_ts_train,p=1,P=1,xreg=fourier(clean_ts_train, K=c(2,2)))
model8.2_forecast <- forecast(model8.2, h=424,xreg=fourier(clean_ts_train, K=c(2,2),h=424))

#Plot foresting results
autoplot(model8_forecast) +
  ylab("load") 

autoplot(model8.1_forecast) +
  ylab("load") 

autoplot(model8.2_forecast) +
  ylab("load") 

#Plot model + observed data
autoplot(clean_ts_test) +
  autolayer(model8.1_forecast, series="Neural Network",PI=FALSE)+
  ylab("Load") 

autoplot(clean_ts_test) +
  autolayer(model8.2_forecast, series="Neural Network",PI=FALSE)+
  ylab("Load") 
```

```{r Score3}
#Model 7: TBATS
model7_scores <- accuracy(model7_forecast$mean, clean_ts_test)

#Model 8: Neural Network 
model8_scores <- accuracy(model8_forecast$mean, clean_ts_test)
model8.1_scores <- accuracy(model8.1_forecast$mean, clean_ts_test)
model8.2_scores <- accuracy(model8.2_forecast$mean, clean_ts_test)
```

```{r Accuracy Test Models 1-8}
model7_scores<- model7_scores[,c("ME","RMSE","MAE","MPE","MAPE")]
model8_scores<- model8_scores[,c("ME","RMSE","MAE","MPE","MAPE")]
model8.1_scores<- model8.1_scores[,c("ME","RMSE","MAE","MPE","MAPE")]
model8.2_scores<- model8.2_scores[,c("ME","RMSE","MAE","MPE","MAPE")]

scores3 <- as.data.frame(rbind(model7_scores, model8_scores, model8.1_scores, model8.2_scores))
scores<- rbind(scores,scores3)

print(scores)

rownames(scores[scores$ME==min(scores$ME),])
rownames(scores[scores$RMSE==min(scores$RMSE),])
rownames(scores[scores$MAE==min(scores$MAE),])
rownames(scores[scores$MPE==min(scores$MPE),])
rownames(scores[scores$MAPE==min(scores$MAPE),])
#looks like models 3, 6.3, 7, and 8.1 are the best
```
