---
title: "TSA Competition"
author: "Angela Zeng & John Rooney"
output:
  html_document:
    df_print: paged
  pdf_document: default
geometry: margin=2.54cm
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,tidy.opts=list(width.cutoff=80), tidy=FALSE) 
```

## Set up
```{r load libraries, message=FALSE}
#Load libraries 
library(readxl)
library(forecast)
library(tseries)
library(smooth)
library(tidyverse)
library(lubridate)
library(Kendall)
library(writexl)
```

##Import and process data 
```{r Data Import and Wrangling}
#import datasets
load_df<- read_excel(path="./Data/load.xlsx",col_names=TRUE)

#Create column with daily load (by averaging hourly load)
load_df$daily_load<- rowMeans(load_df[,3:26])

#Process date column 
load_df$date<- as.Date(load_df$date)
load_df<- load_df %>%
  arrange(date)

#full clean data frame
full_load <- load_df[,c(2,27)]

#create a subset for training purpose (excludes 2010)
train_load<- full_load %>%
  filter(year(date) <= 2009)

#create a subset for testing purpose (only 2010)
test_load<- full_load %>%
  filter(year(date) == 2010)
```

##Create ts object 
```{r Make Training and Test time series}
#full data set
ts_full<- msts(full_load[,2], seasonal.periods =c(7,365.25), start=c(2005,01,01))
#tsclean identifies and replace outliers and missing values using linear interpolation
clean_ts_full <- tsclean(ts_full) 

#excludes 2010, Training set 
ts_train <- msts(train_load[,2], seasonal.periods =c(7,365.25), start=c(2005,01,01))
clean_ts_train <-  tsclean(ts_train)

#only 2010, Test set
ts_test<- msts(test_load[,2], seasonal.periods =c(7,365.25), start=c(2010,01,01))
clean_ts_test <-  tsclean(ts_test)
```

##Plot data 
```{r Initial Plots, ACF and PACF}
#plot the training dataset
ggplot(train_load, aes(x= date, y= daily_load)) +
  geom_line(color= "blue") +
  labs(y='Demand',
       x='Year')+
  scale_x_date(date_breaks = "1 year", date_labels = "%Y")

par(mfrow=c(1,2))
Acf(clean_ts_train,lag.max=40, plot=TRUE)
Pacf(clean_ts_train,lag.max=40, plot=TRUE)
par(mfrow=c(1,1))
```


```{r Decompose, SMK and ADF tests}
#Decompose
decompose_load<- mstl(clean_ts_train)
autoplot(decompose_load)

deseasonal_load <- seasadj(decompose_load) 

#Tests
#if p value > 0.05 then accept null hypothesis, data has a unit root, i.e., stochastic trend
print("Results for ADF test")
print(adf.test(deseasonal_load, alternative = "stationary"))

#if p value < 0.05 then reject null hypothesis, data follow a trend
SMKtest <- SeasonalMannKendall(deseasonal_load)
print("Results for Seasonal Mann Kendall")
print(summary(SMKtest))
```

```{r Model 1 ARIMA}
# Create Model 1 Auto Arima on deseasonalized data, forecast and plot
model1 <- auto.arima(deseasonal_load, max.D=0, max.P=0, max.Q=0)
print(model1)

model1_forecast <- forecast(object = model1, h=424)
plot(model1_forecast)

#AIC of 27337.23
```

```{r Model 2 SARIMA}
#Create Model 2 SARIMA on original data, forecast and plot
model2 <- auto.arima(clean_ts_train)
print(model2)

model2_forecast <- forecast(object = model2, h=424)
plot(model2_forecast)

#AIC of 27951.64
```

```{r Model 3 SES}
#model 3 Simple Exponential Smoothing on original data
model3 <- es(y=clean_ts_train, h=424, holdout=F, silent=F)
print(model3)

checkresiduals(model3)

#AIC of 28079.91
```

```{r Accuracy Test Models 1-3}
#test the accuracy of the models 
model1_scores <- accuracy(model1_forecast, clean_ts_test, test=NULL)
model1_scores<- model1_scores[,c("RMSE","MAPE")]
model2_scores <- accuracy(model2_forecast, clean_ts_test, test=NULL)
model2_scores<- model2_scores[,c("RMSE","MAPE")]
model3_scores <- accuracy(model3$forecast, clean_ts_test, test=NULL)
model3_scores<- model3_scores[,c("RMSE","MAPE")]

scores <- as.data.frame(rbind(model1_scores[2,], model2_scores[2,], model3_scores))
#model 2 is the best
```

```{r Extract data for first Kaggle submission}
#extract jan and feb from 2011 forecast of model2
answer<- model2_forecast$mean[366:424]
answer<- as.data.frame(answer)
#write_xlsx(answer, "answer1.xlsx")
```

```{r Model 4 SNAIVE}
model4 <- snaive(clean_ts_train, h=424, holdout=FALSE)
checkresiduals(model4)
plot(model4)
```

```{r Model 5 ETS}
#Fit and forecast STL + ETS model to data
model5 <-  stlf(clean_ts_train, h=424)

#Plot foresting results
autoplot(model5)
```

```{r Model 6 ARIMA + FOURIER terms}
#Fit arima model with fourier terms as exogenous regressors
# seasonal = FALSE is the same as P=D=Q=0
# play with K by changing it to K=c(2,2), K=c(2,4), K=c(2,6), etc. The higher the K the longer it will take to converge, because R will try more models.

model6 <- auto.arima(clean_ts_train, seasonal=FALSE, lambda=0, 
                     xreg=fourier(clean_ts_train, K=c(2,12)))

#Forecast with ARIMA fit
#also need to specify h for fourier terms
model6_forecast <- forecast(model6,
                            xreg=fourier(clean_ts_train, K=c(2,12), h=424),
                            h=424) 

#Plot foresting results
autoplot(model6_forecast)
```

```{r Model 6.2 ARIMA + FOURIER terms}
#Fit arima model with fourier terms as exogenous regressors
# seasonal = FALSE is the same as P=D=Q=0
# play with K by changing it to K=c(2,2), K=c(2,4), K=c(2,6), etc. The higher the K the longer it will take to converge, because R will try more models.

model6.2 <- auto.arima(clean_ts_train, seasonal=FALSE, lambda=0, 
                     xreg=fourier(clean_ts_train, K=c(2,6)))

#Forecast with ARIMA fit
#also need to specify h for fourier terms
model6.2_forecast <- forecast(model6.2,
                            xreg=fourier(clean_ts_train, K=c(2,6), h=424),
                            h=424) 

#Plot foresting results
autoplot(model6.2_forecast)+ylim(0,10000) #with ylim, difficult to see the massive confidence interval
```

```{r Model 6.3 ARIMA + FOURIER terms}
#Fit arima model with fourier terms as exogenous regressors
# seasonal = FALSE is the same as P=D=Q=0
# play with K by changing it to K=c(2,2), K=c(2,4), K=c(2,6), etc. The higher the K the longer it will take to converge, because R will try more models.

model6.3 <- auto.arima(clean_ts_train, seasonal=FALSE, lambda=0, 
                     xreg=fourier(clean_ts_train, K=c(2,4)))

#Forecast with ARIMA fit
#also need to specify h for fourier terms
model6.3_forecast <- forecast(model6.3,
                            xreg=fourier(clean_ts_train, K=c(2,4), h=424),
                            h=424) 

#Plot foresting results
autoplot(model6.3_forecast)+ylim(0,10000) #with ylim, difficult to see the massive confidence interval
```

```{r Model 6.4 ARIMA + FOURIER terms}
#Fit arima model with fourier terms as exogenous regressors
# seasonal = FALSE is the same as P=D=Q=0
# play with K by changing it to K=c(2,2), K=c(2,4), K=c(2,6), etc. The higher the K the longer it will take to converge, because R will try more models.

model6.4 <- auto.arima(clean_ts_train, seasonal=FALSE, lambda=0, 
                     xreg=fourier(clean_ts_train, K=c(2,2)))

#Forecast with ARIMA fit
#also need to specify h for fourier terms
model6.4_forecast <- forecast(model6.4,
                            xreg=fourier(clean_ts_train, K=c(2,2), h=424),
                            h=424) 

#Plot foresting results
autoplot(model6.4_forecast) #with ylim, difficult to see the massive confidence interval
```

```{r}
#Model 4: Seasonal naive 
model4_scores <- accuracy(model4$mean,clean_ts_test)

#Model 5: STL + ETS
model5_scores <- accuracy(model5$mean, clean_ts_test)  

#Model 6: ARIMA + Fourier 
model6_scores <- accuracy(model6_forecast$mean, clean_ts_test)

model6.2_scores <- accuracy(model6.2_forecast$mean, clean_ts_test)
model6.3_scores <- accuracy(model6.3_forecast$mean, clean_ts_test)
model6.4_scores <- accuracy(model6.3_forecast$mean, clean_ts_test)
```

```{r Accuracy Test Models 1-6}
model4_scores<- model4_scores[,c("RMSE","MAPE")]
model5_scores<- model5_scores[,c("RMSE","MAPE")]
model6_scores<- model6_scores[,c("RMSE","MAPE")]
model6.2_scores<- model6.2_scores[,c("RMSE","MAPE")]
model6.3_scores<- model6.3_scores[,c("RMSE","MAPE")]
model6.4_scores<- model6.4_scores[,c("RMSE","MAPE")]

scores2 <- as.data.frame(rbind(model4_scores, model5_scores, model6_scores, model6.2_scores, model6.3_scores, model6.4_scores))

scores<- rbind(scores,scores2)

#looks like models 6.3 and 6.4 are the same and the best models
```

```{r Extract data for second Kaggle submission}
#extract jan and feb from 2011 forecast of model6.3
answer2<- model6.3_forecast$mean[366:424]
answer2<- as.data.frame(answer2)
#write_xlsx(answer2, "answer2.xlsx")
```